%!TEX root = ../username.tex
\chapter{Developing with Vuforia}\label{vuforia}
Like most AR platforms, Vuforia's functionality revolves around the concepts of marker detection and tracking. The main difference between Vuforia and other prominent AR development platforms is its focus on customized markers, or targets, as the Vuforia documentations call them. Users are allowed to register their own targets on an online database and then download the target database in a package that can be imported into Unity or deployed in Visual Studio, Xcode or other integrated development environments (IDE). This chapter provides an overview of the functionality of the Vuforia framework and introduces the use of Vuforia with Unity. Section~\ref{workflow} discusses the complete workflow of developing an AR mobile application using Vuforia and Unity.

\section{Features}
As previously noted, the prominent feature of Vuforia is its ability to track customized targets. Figure~\ref{targets} lists the targets that can be tracked in a Vuforia-enabled app. In Vuforia terminology, all of these targets are considered \texttt{Trackable}s. Recall the definition of interest points in the previous discussion on feature detection. A \texttt{Trackable} is a set of interest points that can be recognized and tracked at run time. Since the targets are customized, these interest points are preregistered offline and stored in a database. Interest points detected at run time are compared against this database to identify the targets. Once a target is matched, its pose is retrieved and the orientation and position data are used to align virtual contents to the physical targets. These targets are divided into three main categories: images, objects, and environments. Each category is further divided into several smaller subgroups:

\begin{figure}[!ht]
\rightline{
\begin{minipage}{\textwidth}
\begin{center}
\woopic{targets}{0.3}
\vspace{-.2 in}
\caption{Vuforia targets. Source: Vuforia developer portal}\label{targets}
\end{center}
\end{minipage}
}
\end{figure}

\begin{enumerate}
\item Image targets

An image target is a flat image that Vuforia can detect and track. The engine uses natural feature detection to select the interest points from the camera feed and match them to a known database of targets. Vuforia is capable of tracking a detected image as long as it is partially visible to the camera. Usually, virtual contents are attached to the image target (see Figure~\ref{teapot} for an example of a virtual teapot attached to an image target). Its most common use cases are for augmenting product packaging and posters. (Vuforia)

\begin{figure}[!ht]
\rightline{
\begin{minipage}{\textwidth}
\begin{center}
\woopic{teapot}{0.5}
\vspace{-.2 in}
\caption[Image target]{A detected image target with a virtual teapot attached. Source: Vuforia developer portal}\label{teapot}
\end{center}
\end{minipage}
}
\end{figure}

\begin{figure}[!ht]
\rightline{
\begin{minipage}{\textwidth}
\begin{center}
\woopic{cuboid}{0.5}
\vspace{-.2 in}
\caption[Multi target]{A box can serve as a multi target. Source: Vuforia developer portal}\label{cuboid}
\end{center}
\end{minipage}
}
\end{figure}

\item Multi targets

A multi target is a set of image targets with a predefined geometric arrangement. An example of a multi target is the box in Figure~\ref{cuboid}. The box is comprised of six images (one for each face), each of which is a separate image target. The pose of these image targets are defined relative to one another. When detecting a multi target, the identified interest points must not only match the image targets, but their pose must match the predetermined geometric arrangement.

\item Cylinder targets

A cylinder target is an extension of an image target, wrapped around a cylindrical or conical object. The object's dimensions (diameter and size length) must be defined. The system can also track the images on the top and bottom faces of the cylindrical target if defined.

\item Object recognition

The object recognition feature of Vuforia allows users to create trackable targets from physical objects. The Vuforia Object Scanner creates a digital representation of a real-life 3D object by extrapolating the features and geometry of the object from the scan. Unlike image targets, object targets do not have to be planar. Figure~\ref{object_scanner} shows an example of the result of scanning a toy car. 

\begin{figure}[!ht]
\rightline{
\begin{minipage}{\textwidth}
\begin{center}
\woopic{object_scanner}{0.5}
\vspace{-.2 in}
\caption[Vuforia Object Scanner]{The Vuforia Object Scanner picks up the features and shape of a scanned toy car.}\label{object_scanner}
\end{center}
\end{minipage}
}
\end{figure}

\item VuMark

\begin{figure}[!ht]
\rightline{
\begin{minipage}{\textwidth}
\begin{center}
\woopic{vumark.png}{1.}
\vspace{-.2 in}
\caption{VuMark}\label{vumark}
\end{center}
\end{minipage}
}
\end{figure}

VuMarks (Figure~\ref{vumark}) are Vuforia's version of marker-based tracking. The existence of both image targets and VuMarks can be confusing, but VuMark designs allow for the coexistence of millions of trackable instances as well as the ability to encode a variety of data formats. Because of the uniqueness of each VuMark instance, these markers can also be used as a means of identification among similar products.

\item Model targets

Model targets are targets that are created from a digital 3D model of an object and can be used to track it. The Model Target Generator converts 3D object files into Vuforia Engine databases. Vuforia supports the real-time tracking of the registered models in the databases. In order to initiate the tracking, the Vuforia app must detect the object from a specific angle, prompted by the guide view (see Figure~\ref{guideview} for a screen capture of the guide view).

\begin{figure}[!ht]
\rightline{
\begin{minipage}{\textwidth}
\begin{center}
\woopic{guideview}{0.5}
\vspace{-.2 in}
\caption{Model Target Guide View}\label{guideview}
\end{center}
\end{minipage}
}
\end{figure}

\item Extended tracking

The main problem with tracking targets is that pose calculation desists when the target is no longer visible in the frame. Vuforia offers a solution to this issue in the form of extended tracking. Extended tracking makes use of a Device Tracker feature to enable six-degrees-of-freedom tracking. The Device Tracker provides information about the device's pose at all times relative to the targets and the rest of the world. After the device's position has been established the first time a target is detected, alignment between real-world objects and virtual contents persists even after the camera has moved from the target's position. Extended tracking makes space-aware AR experiences possible, for example games that require lots of space or when tracking a large object.

\item Ground plane

The ground plane detector identifies horizontal surfaces and allows users to place content on top of the plane or elsewhere relative to the detected plane. The Mid Air Stage places virtual content in mid air based on its preset position from the target ground plane. Vuforia ground plane detection is enabled by platforms such as ARCore and ARKit.

\end{enumerate}

The functionality of Vuforia is based around its ability to recognize different kinds of targets. The next section explains the back-end framework that enables tracking for a variety of objects.

\section{Core Components}
\begin{figure}[!ht]
\rightline{
\begin{minipage}{\textwidth}
\begin{center}
\woopic{vuforia.png}{.32}
\vspace{-.2 in}
\caption[Vuforia core classes]{UML diagram of Vuforia core classes for Unity}\label{coreclass}
\end{center}
\end{minipage}
}
\end{figure}

An application created with Vuforia in Unity requires several core components. The Unified Modeling Language (UML) class diagram in Figure~\ref{coreclass} shows the class hierarchy of the core of Vuforia. Consider a simple AR app that scans an image target and overlays the target with a 3D model. As the app starts, a new AR session is initiated. The \texttt{VuforiaARController} class stores the information about the new AR session. At the beginning of the session, information about the camera is registered in \texttt{CameraDevice}. During each frame, the image captured by the camera feed can be obtained from \texttt{CameraDevice} as well. Each picture frame is then passed on to the \texttt{TargetFinder}, which scans the image for features and prepares it for matching using a separate process of object recognition. This process is represented by the class \texttt{ObjectRecoBehaviour}. Notice that \texttt{ObjectRecoBehaviour} is a child of \texttt{VuforiaMonoBehaviour}, which is derived from \texttt{MonoBehaviour}, the base class for all Unity scripts, which offers functions for handling the life cycles of an app. For every single object/image that is found by \texttt{TargetFinder}, the result is compared against a reference database by \texttt{ObjectRecoBehaviour}. If the result matched a preregistered target, its position and orientation are assigned to the Unity game object that represents the target. When a target is located and paired to its representation, a corresponding \texttt{ObjectTarget} is created. An \texttt{ObjectTarget} inherits the base class behavior \texttt{Trackable} for all trackable types in Vuforia. The classes for the different target types (e.g. \texttt{ImageTarget}, \texttt{MultiTarget}, etc.) are all subclasses of  \texttt{ObjectTarget}. The appropriate subclass is chosen to match the kind of target identified by \texttt{TargetFinder}.

All of these components are embedded in the Vuforia framework and are not included in the actual developing workflow. Users designing a Vuforia app with Unity can use Unity's graphical user interface to interact with these components at the graphical level.

\section{Workflow}\label{workflow}
The workflow of designing a Vuforia-enabled app in Unity follows the general workflow of the Unity game engine. This section highlights the Vuforia-specific aspects of designing an AR app in Unity.
\subsection{Installation and Activation}
The Vuforia Engine is available through two channels in Unity, as a package downloadable from the Package Manager (for Unity 2019.2 or later) and as a separate component that can be installed using the Unity Download Assistant (for Unity versions before 2019.2). Activation of the Vuforia Engine can be done by enabling Vuforia Augmented Reality Supported in Player Settings. Activation is successful when Vuforia appears in the GameObject menu (Figure~\ref{menu}).

\begin{figure}[!ht]
\rightline{
\begin{minipage}{\textwidth}
\begin{center}
\woopic{vuforiamenu.png}{.35}
\vspace{-.2 in}
\caption{Vuforia submenu}\label{menu}
\end{center}
\end{minipage}
}
\end{figure}

\subsection{Creating Target Databases}
To be able to use custom targets in Unity, developers have to create the target database(s) and import them as an asset package. This process can be done on the Developer Portal on the Vuforia Engine website. The Target Manager function has buttons for creating new databases. There are two options for storing a database, on the cloud or offline. Since the Cloud Recognition feature is for enterprise use only, this section only explains the offline mode, which is called Device on the portal. The Add Target button is for uploading a new target. Among the different target types, there are five that can be customized: image, cuboid multi target, cylinder, object, and model. Model target databases are created using the Model Target Generator. The Target Manager deals with images, multi targets, cylinders, and objects. After the file has been uploaded, the target's properties should be changed to match its actual dimensions. For all image targets, the Manger gives each target a rating of 0 to 5. A 5-star rating means that the image is highly augmentable. These are images that are rich in detail, have good contrast, and include no repetitive patterns. Figure~\ref{ratings} include two examples of a 5-star and 0-star image targets. Figure~\ref{5star} has great contrast between bright and dark areas, while Figure~\ref{0star} is too repetitive and has uneven feature distribution. The Vuforia Target Manager has a feature visualizer for image targets that are useful for understanding how well feature detection works for different images. See Figure~\ref{feature_ratings} for the features detected (marked yellow) in the two images in Figure~\ref{ratings}. The 0-star image target has much sparser features than the 5-star rating one, making it less recognizable. As a result, optimizing the trackability of image targets is important.

\begin{figure}[!ht]\centering
\subfigure[5-star rating]
{\woopic{5star}{.3}\label{5star}}
\qquad
\subfigure[0-star rating]
{\woopic{0star}{.3}\label{0star}}
\vspace{-.2 in}
\caption{Image target ratings}\label{ratings}
\end{figure}

\begin{figure}[!ht]\centering
\subfigure[5-star rating]
{\woopic{feature_5star.png}{.57}\label{feature_5star}}
\qquad
\subfigure[0-star rating]
{\woopic{feature_0star.png}{.57}\label{feature_0star}}
\vspace{-.2 in}
\caption{Vuforia feature detection}\label{feature_ratings}
\end{figure}

Because of the nature of maps, this project faces several challenges with trackability. Because maps have many repetitive details (lots of similar symbols) and not much contrast (no extreme brightness difference), it is harder to optimize maps. There are several ways to prepare images for better target detection. The actual process of optimizing a 2005 map of Ho Chi Minh City used in this project is described in Figure~\ref{optimize}. The original map in Figure~\ref{preop} has very few distinguishable interest points and poor contrast. In version~\ref{postop1}, the contrast of the map was increased by adding a black border and lightening the pink and green areas. This action increases the local contrast between darker details such as roads, and the brighter background. To achieve the 4-star rating in Figure~\ref{postop2}, the foreground/background contrast was further enhanced and repetitive features were removed. However, the optimization process has a major effect on the integrity of the 4-star image because it eliminates some important information on the map. Due to the excessive alteration to the 4-star image, it makes to choose the 2-star image, which still yields a robust performance for iOS devices. 

\begin{figure}[!ht]\centering
\subfigure[Before optimizing (0-star)]
{\woopic{map_2005}{.4}\label{preop}}
\qquad
\subfigure[After adding border and increasing local contrast (2-star)]
{\woopic{improve1}{.09}\label{postop1}}
\subfigure[After increasing foreground/background contrast and eliminating repetitiveness (4-star)]
{\woopic{improve2.png}{.3}\label{postop2}}
\vspace{-.2 in}
\caption{Optimizing image target for detections}\label{optimize}
\end{figure}

In addition to images, users can also upload 3D object scans to create object targets. The Vuforia Object Scanner is an Android application that makes object targets from $360^o$ scans of an object. The Object Scanner generates the data necessary to define an object target. Preparing the scan environment is important for obtaining a good model. The lighting in the environment should be diffuse and sufficiently bright, without any directional light sources. This should prevent specular highlights, which lead to false interest points. A cluttered background also flags false points. The object space of the model is defined relative to the coordinates provided by a target image (Figure~\ref{scanner_target}). The object to be scanned must be placed in the grey box in the top left corner of the target image. The box is delimited by the axes defining the object space. Any part of the object outside of the space delimited by the grids is culled. The pose of the object is calculated relative to the pose of the target image. The principles for detecting an object are generally similar to the ones used in feature point detection. A decent number of interest points is important, as well as good coverage. When scanning an object, the app provides the visualization of the coverage of the interest points detected. After the scanner has established a general shape for an object, a mesh that covers the object appears around it. Regions that have the sufficient number of points detected is colored green. Regions unscanned or have sparse interest point distributions are not colored. Found interest points are represented by the green dots. To improve the model, scanning should be repeated twice with the target image replaced by a dark and a light background.

\begin{figure}[!ht]
\rightline{
\begin{minipage}{\textwidth}
\begin{center}
\woopic{obj_scanner_target}{.3}
\vspace{-.2 in}
\caption{Object Scanner target image. From: Vuforia}\label{scanner_target}
\end{center}
\end{minipage}
}
\end{figure}

\begin{figure}[!ht]
\rightline{
\begin{minipage}{\textwidth}
\begin{center}
\woopic{lipstick_scan}{.15}
\vspace{-.2 in}
\caption{A scan in progress. From: author}\label{scan}
\end{center}
\end{minipage}
}
\end{figure}

The resulting file generated by the Object Scanner is an Object Data (*.OD) file. OD files can be uploaded using the online Target Manager to create object target databases. An object target is treated the same way as an image target in Unity. In the Unity hierarchy, children of the object target \textbf{GameObject} are its augmented contents.

After all the targets have been added, the database is ready for download. The Unity Editor option allows the package to be imported and extracted like any other asset package in Unity.

\subsection{Creating a Scene}
All features of Vuforia are accessible as a \textbf{GameObject}. A \textbf{GameObject} in Unity is any object in a scene, from lights to cameras to characters (cite Unity GameObjects). A default scene in Unity includes a default camera \textbf{MainCamera}. However, as the position and orientation of the camera in an AR app depends on the actual pose of the device's camera, the \textbf{MainCamera} has to be replaced by an \textbf{ARCamera}, found in the Vuforia Engine under the GameObject menu. The \textbf{ARCamera} is configured to simulate the pose of the actual device's camera. To make the app detect a target, the appropriate target GameObject needs to added to the scene. For instance, the Image GameObject creates an Image Target for tracking. The imported target database is selected in the Inspector window under the Image Target Behaviour component. The properties of the \textbf{ImageTarget} object have been already preconfigured. Unless users want to add more properties, the target is now ready for tracking. To place other virtual contents in relation to the position of the target,  the relevant \textbf{GameObjects} must be added as children of the target. This hierarchy ensures that the children objects' pose are always calculated relative to the detected target. This AR app is now deployable with one scene that tracks a single target and overlays it with the chosen virtual contents.

\subsection{Creating Augmented Contents}
As discussed, the augmented contents can be any 3D \textbf{GameObject} in Unity. This project is concerned with two main types of \textbf{GameObject}s, quads and 3D models. Quads are used to create augmented maps, textured with the actual map image. The 3D models used are COLLADA (*.dae) files, downloaded from Sketchfab and imported as assets into Unity. One of the models was created using photogrammetry, a technique that produces 3D models of objects and scenes from raw photographs. The software used to create this model is Meshroom, an open-source 3D reconstruction software.

\section{Digital Storytelling}
In addition to the technical aspect of creating an AR experience, the narrative is another important component for producing immersive content. Digital storytelling describes a form of narrative that uses technology and digital media in its production.\footnote{https://learning.oreilly.com/library/view/digital-storytelling-3rd/9780415836944/xhtml/11\_Ch1.xhtml} An AR application is a digital story which utilizers techniques in computer vision and graphics to create an experience. As a result, the AR narrative has to follow the same principles as traditional storytelling, which entails having a clear goal, an established structure and format, with consistent and logical behaviors and events.\footnote{https://learning.oreilly.com/library/view/digital-storytelling-3rd/9780415836944/xhtml/16\_Ch5.xhtml} When adding in the interactive components of storytelling, elements such as interface, navigation, point of view, gameplay and use of time and space. The process of creating an AR experience, whether done in Unity with Vuforia or on any other platforms, must take these considerations into account when designing the narrative.

A historical exhibit is also a form of storytelling. An AR-enabled historical exhibit performs digital storytelling through a screen medium. The goal of the AR experience in this project is to demonstrate how the multi-layer memory of Ho Chi Minh City is embedded in objects and spaces such as maps and buildings. To achieve this goal, the experience is structured around physical and virtual exhibit items. Both categories are interactive. Physical items are wall and table-top maps and 3D-printed models of buildings. These objects, specifically the 3D-printed models, allow users all the usual interactions; they can be picked up, rotated, and moved around. The AR virtual contents also have interactions that users can perform through the mobile device's screen, like dragging and dropping for moving them or pinching for zooming. An AR narrative progresses by prompting user interactions with visual and audio cues, both onscreen and offscreen. The audio cues in this project are provided by an audio playback during the experience, which advances the narrative through traditional (oral) storytelling and provides guidance for AR interactions. 

An important consideration when designing a digital story is deciding whether an immersive treatment is going to enhance it.\footnote{https://speakerdeck.com/rapoulson/vr-workshop-2019?slide=48} Physical immersion should only be employed if it makes the narrative more effective. For this project specifically, what augmented reality brings that is otherwise impossible is perspective agency. Part of the argument of this historical research is that the recollections provoked by cartography and architecture do not allow much freedom on the side of the citizens of Ho Chi Minh City. The exhibit was produced with the hope that it elicits a different kind of recollection that is critical and context-sensitive. Augmented reality supports that freedom, providing users with the agency to choose their perspectives. Through engagement with and manipulation of space and virtual objects, the experience encourages users to think critically about who is manipulating these sites in reality, how memory is engendered, and what technology has assisted this process of memory formation.